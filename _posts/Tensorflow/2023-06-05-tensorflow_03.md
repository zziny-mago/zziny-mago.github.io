---
layout: post
title:  "Model Implementation"
subtitle: "python,데이터 사이언티스트, 인공지능 개발자를 위함"
categories: Tensorflow
tags: Tensorflow
comments: true
---


## Sequebtial
~~~
    x_train=tf.random.normal(shape=(1000,1),dtype=tf.float32)
    y_train=3*x_train+1+0.2*tf.random.normal(shape=(1000,1),dtype=tf.float32)


    x_test=tf.random.normal(shape=(300,1),dtype=tf.float32)
    y_test=3*x_test+1+0.2*tf.random.normal(shape=(300,1),dtype=tf.float32)

    fig,ax=plt.subplots(figsize=(10,10))
    ax.scatter(x_train.numpy(),
            y_train.numpy())
    ax.tick_params(labelsize=20)
    ax.grid()

~~~
![image](https://github.com/tired-o/test/assets/70193130/f4f59c27-7dae-495e-8ca9-7c0e36f57b27)


+ t3에 대한 t1,t2 편미분 값
+ tape의 경우 dtype=tf.float32 설정 해야함

~~~
    model=tf.keras.models.Sequential([
        tf.keras.layers.Dense(units=1,activation='linear')
    ])

    model.compile(loss='mean_squared_error',
                optimizer='SGD')  

    model.fit(x_train,y_train,epochs=10)
    print()
    model.evaluate(x_test,y_test)
~~~

+ t2의 경우 chain rule에 의해 나온 값

## Model Sub Classing

~~~
    x_train=tf.random.normal(shape=(10,1),dtype=tf.float32)
    y_train=3*x_train+1+0.2*tf.random.normal(shape=(10,1),dtype=tf.float32)


    x_test=tf.random.normal(shape=(300,1),dtype=tf.float32)
    y_test=3*x_test+1+0.2*tf.random.normal(shape=(300,1),dtype=tf.float32)
~~~

+ constant 텐서는 기록되지 않음


~~~
    class LinearPredictor(tf.keras.Model):
        def __init__(self):
            super(LinearPredictor,self).__init__()
            self.d1=tf.keras.layers.Dense(units=1,activation='linear')
        
        def call(self,x):
            x=self.d1(x)
            return x

    EPOCHS=10
    LR=0.01

    model=LinearPredictor()
    criterion=tf.keras.losses.MeanSquaredError()
    optimizer=tf.keras.optimizers.SGD(learning_rate=LR)

~~~

~~~
    for epoch in range(EPOCHS):
        for x,y in zip(x_train,y_train):
            x=tf.reshape(x,(1,1))
            with tf.GradientTape() as tape:
                predictions=model(x)
                loss=criterion(y,predictions)
            gradients=tape.gradient(loss,model.trainable_variables)
            optimizer.apply_gradients(zip(gradients,model.trainable_variables))
        print(epoch+1)
        print(loss)

    1
    tf.Tensor(30.546247, shape=(), dtype=float32)
    2
    tf.Tensor(17.309853, shape=(), dtype=float32)
    3
    tf.Tensor(10.248536, shape=(), dtype=float32)
    4
    tf.Tensor(6.353091, shape=(), dtype=float32)
    5
    tf.Tensor(4.123877, shape=(), dtype=float32)
    6
    tf.Tensor(2.7975774, shape=(), dtype=float32)
    7
    tf.Tensor(1.9762115, shape=(), dtype=float32)
    8
    tf.Tensor(1.4467182, shape=(), dtype=float32)
    9
    tf.Tensor(1.091786, shape=(), dtype=float32)
    10
    tf.Tensor(0.8449189, shape=(), dtype=float32)
~~~


