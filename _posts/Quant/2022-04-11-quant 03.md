---
layout: post
title:  "마르코프 의사 결정 과정"
subtitle: "python,데이터 사이언티스트, 인공지능 개발자를 위함"
categories: quant
tags: quant
comments: true
---
### 마르코프 가정 & 과정
---
연속적인 상태가 이어질때 어떤 시점에서 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정 -> 어려운 문제들을 단순화 하고 만족스러운 결과를 도출

+ 의사 결정과정에서 가질 수 있는 모든 상태집합 S 
+ 행동 주체인 에이전트가 할 수 있는 모든 행동의 집합 A

![image](https://user-images.githubusercontent.com/70193130/162660592-5ef38e25-1869-45ac-991b-d0faa0baca7f.png)

어떠한 상태 s에서 행동 a를 취했을때 상태 s'로 변할 확률
(전이확률)


![image](https://user-images.githubusercontent.com/70193130/162660598-8bdf14d7-4b4e-4778-96d3-38eb6563fc8e.png)

상태 s에서 행동 a를 했을 때의 보상값(보상함수)


![image](https://user-images.githubusercontent.com/70193130/162660557-77e1c03f-e653-4634-bf22-ef03668d7f93.png)

에이전트는 어떠한 상태에서 할 행동을 결정해야함 이것을 정책이라고 부름
정책은 학습을 함에 따라 보상이 최대화 되는쪽으로 감


![image](https://user-images.githubusercontent.com/70193130/162660582-b09349b4-3c95-45b4-a64b-ab884b3f7444.png)


출발점 s1상태에서 함정에 빠지지 않고 목적지s9 상태로 보상을 줘가며 가는것이 목표

상태전이확률은 에이전트의 결정에 무조건 상태가 변하도록 -> 확률을 1로 설정 (여기서 에이전트의 결정은 정책!)

![image](https://user-images.githubusercontent.com/70193130/162660602-8c1475aa-5090-47b0-accb-f06a66b6434d.png)


s3 함정에 도착-> 직전의 상태는 s2 보상을 그대로 받음
-> s1상태는 보상에 할인 요인을 적용시켜 반영(먼 과거에 보상일 수록 깎아서 반영)